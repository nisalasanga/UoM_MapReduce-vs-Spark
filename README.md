# MapReduce vs Spark
MapReduce and Spark are two popular technologies used for processing and analyzing large data sets. Both MapReduce and Spark are designed for distributed processing of large data sets. Here we are interested in comparing these Two technologies with run time and ease of use.

# MapReduce
The MapReduce is a model consists of several steps that are executed in sequence to process and generate large datasets in a parallel and distributed manner.
The MapReduce process is typically executed in a distributed computing environment, where multiple machines work together to process the input dataset. The framework manages the distribution of the input data, the execution of map and reduce tasks, and the communication between nodes in the cluster.

![The-Architecture-of-MapReduce](https://user-images.githubusercontent.com/63199917/224222081-47e41ca4-cccd-495b-8562-a198fa1475b1.jpg)

As shown in the above figure mapreduce contain 5 steps.

##### Input Splitting: 
The input dataset is divided into smaller chunks, called input splits. Each input split is processed independently by a map task.

##### Mapping: 
Each input split is processed by a map task, which applies a user-defined map function to each record in the input split. The map function transforms the input record into intermediate key-value pairs.

##### Shuffling and Sorting: 
The intermediate key-value pairs generated by the map function are shuffled and sorted based on their keys. The shuffle and sort process groups all the intermediate key-value pairs with the same key and sends them to the same reducer task.

##### Reducing: 
The reducer task processes each group of intermediate key-value pairs with the same key. The reducer applies a user-defined reduce function to the values associated with each key, which generates the final output values.

##### Output: 
The final output values generated by the reducer task are written to the output file or storage system.

# Apache Spark 
