# MapReduce vs Spark
MapReduce and Spark are two popular technologies used for processing and analyzing large data sets. Both MapReduce and Spark are designed for distributed processing of large data sets. Here we are interested in comparing these Two technologies with run time and ease of use.

# MapReduce
The MapReduce is a model consists of several steps that are executed in sequence to process and generate large datasets in a parallel and distributed manner.
The MapReduce process is typically executed in a distributed computing environment, where multiple machines work together to process the input dataset. The framework manages the distribution of the input data, the execution of map and reduce tasks, and the communication between nodes in the cluster.

![The-Architecture-of-MapReduce](https://user-images.githubusercontent.com/63199917/224222081-47e41ca4-cccd-495b-8562-a198fa1475b1.jpg)

As shown in the above figure mapreduce contain 5 steps.

##### Input Splitting: 
The input dataset is divided into smaller chunks, called input splits. Each input split is processed independently by a map task.

##### Mapping: 
Each input split is processed by a map task, which applies a user-defined map function to each record in the input split. The map function transforms the input record into intermediate key-value pairs.

##### Shuffling and Sorting: 
The intermediate key-value pairs generated by the map function are shuffled and sorted based on their keys. The shuffle and sort process groups all the intermediate key-value pairs with the same key and sends them to the same reducer task.

##### Reducing: 
The reducer task processes each group of intermediate key-value pairs with the same key. The reducer applies a user-defined reduce function to the values associated with each key, which generates the final output values.

##### Output: 
The final output values generated by the reducer task are written to the output file or storage system.

# Apache Spark 
Apache Spark is an open-source distributed computing system that is designed to process large amounts of data in parallel across multiple nodes in a cluster. 
Spark provides a unified framework for processing and analyzing data from a variety of sources, including batch processing, streaming data, machine learning, and graph processing. It achieves high performance by keeping data in memory and optimizing data processing across distributed computing nodes.

Spark includes a variety of components, including:

![5fa7ecda6639f6566345729b_Apache Spark Ecosystem PNG-p-1080](https://user-images.githubusercontent.com/63199917/224227452-f256e191-9f04-49e1-9dad-94c1628ea1dd.png)

##### Spark Core: 
The basic building block of the Spark platform, which provides the distributed task scheduling and data distribution functionality that underlies all Spark applications.

##### Spark SQL: 
a module that provides a programming interface for working with structured and semi-structured data using SQL queries and data frames.

##### Spark Streaming: a module for processing live data streams in real time.
MLlib: a library of machine learning algorithms for classification, regression, clustering, and collaborative filtering.

##### GraphX: a library for graph processing and analysis.

Also, Spark can be used with a variety of programming languages, including Java, Scala, Python, and R. It can run on a variety of cluster managers, including Hadoop YARN, Apache Mesos, and Kubernetes.

# Loading, Processing and Quering data using MapReduce And Spark

## Dataset

The below provided dataset, contain up to 1.936.758 different internal flights in the US for 2008 and their causes for delay, diversion and cancellation; if any.

https://www.kaggle.com/code/adveros/flight-delay-eda-exploratory-data-analysis/notebook
However we have used a sample of this dataset for the analysis.

## Analysis
To store the data we used Amazon s3 and for the Analysis, We used Amazon Emr.

### Storing Data using Amazon s3

First thing is go to s3 and create a bucket

<img width="948" alt="S3 bucket" src="https://user-images.githubusercontent.com/63199917/224231105-8d55cb1b-907a-4fd8-982d-d6c822bddc03.png">

After that go inside that bucket and create a folder. Then upload the data to that folder.

<img width="804" alt="s3 folder" src="https://user-images.githubusercontent.com/63199917/224232027-2d7f2c17-6695-43ba-a4de-6f3a71f8d3f8.png">

### Crating a cluster Using Amazon EMR

Go to Amazon Emr and click create cluster.
Then go to advanced settings and choose folowing cinfigurations.
Here we have use hive and spark both for fair evaluation (same resourses)  

<img width="806" alt="emr cluster" src="https://user-images.githubusercontent.com/63199917/224232533-911f8b7d-f334-45b4-87a3-d4a5a49fb9f9.png">

Click next.
In the Hadware configuration section choose x4.large for master and core nodes.

<img width="803" alt="Hardwaare config" src="https://user-images.githubusercontent.com/63199917/224232839-2b29981c-79d8-4b13-90a1-2cb24eda18d8.png">

Click next

again click next

In the security section select your Ec2 key pair.

Click Create cluster.

